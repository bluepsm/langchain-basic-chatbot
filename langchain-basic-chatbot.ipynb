{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e634faa-e75e-4ef3-874c-48e0e718278a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain langchain_community\n",
    "%pip install -qU langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d687ff-c760-45ed-9a4b-7e670fb9b8d2",
   "metadata": {},
   "source": [
    "Set environment keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15f35f9-b901-4660-bf87-83e341d382f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"basic-chatbot-gemini\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac725c7-7707-449b-851b-851c71467bed",
   "metadata": {},
   "source": [
    "Set model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c752944d-d6f4-481d-bd72-7cbb67baa8da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "model = ChatVertexAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b08da2-e8f9-4c1d-8013-e2c57a615e16",
   "metadata": {},
   "source": [
    "Create empty history store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71fe121a-8e8f-4b8e-99c8-493079a520fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8ec4b-2221-4d77-a1e0-26f5c4f579dc",
   "metadata": {},
   "source": [
    "Create prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5308180-8909-4dd3-bc8f-faf4f41467c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9a53df-0651-4776-a266-54ae1a0c4f1a",
   "metadata": {},
   "source": [
    "Create history trimmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fde05984-0fba-4f5b-8f6c-0e763c0ed258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e015a7d-20db-4298-bfb0-2df64f94d5eb",
   "metadata": {},
   "source": [
    "Define module chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b9ddd5-3ecd-4f3c-90ef-d4a644a5b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77365da-ef25-490a-be0d-f071d046c3e2",
   "metadata": {},
   "source": [
    "Define message stream function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "582ae260-feaf-4b78-bd9f-623cc27bbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def stream_message(message, language, config):\n",
    "    for r in with_message_history.stream(\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=message)],\n",
    "            \"language\": language,\n",
    "        },\n",
    "        config=config,\n",
    "    ):\n",
    "        print(r.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e2f7aa-3912-46a0-b959-816d26f3e235",
   "metadata": {},
   "source": [
    "Chat example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5f4778b-263b-4a8b-86fe-fa16273ed7d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi| Blu! ðŸ‘‹ It's nice to meet you. What can I do for| you today? ðŸ˜Š \n",
      "||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"a\"}}\n",
    "\n",
    "stream_message(\"Hi I'm Blu\", \"English\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b9a06ca-c87d-492c-90e1-c6619eeb79c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I| don't really have a name, since I'm just a computer program|.  But you can call me Bard, if you like! ðŸ˜„ \n",
      "\n",
      "|What are you interested in talking about today?  \n",
      "||"
     ]
    }
   ],
   "source": [
    "stream_message(\"Nice to meet you too! What's your name?\", \"English\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "209df904-9aa6-43f4-8474-d786917849ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi| there! ðŸ‘‹  It's nice to hear from you. How are you| doing today? \n",
      "||"
     ]
    }
   ],
   "source": [
    "stream_message(\"Oh, I just came by to say hi\", \"English\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b86297e-9c4b-4424-88e4-0b97909e2567",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As| an AI, I don't have a memory of past conversations.  If| you'd like to tell me your name, I'd be happy to| remember it for this conversation! ðŸ˜Š \n",
      "||"
     ]
    }
   ],
   "source": [
    "stream_message(\"Do you remember my name?\", \"English\", config)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu121.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
